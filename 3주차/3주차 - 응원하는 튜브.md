# 5장. 안정 해시 설계

## 해시 키 재배치 문제

N개의 캐시 서버들에 부하를 균등하게 나누기 위해서는 해시 함수를 사용한다. 이는 서버 풀의 크기가 고정되어 있거나 데이터 분포가 균등할 때는 잘 동작하지만, 만약 장애로 인해 서버의 개수가 변경되거나 하는 상황이 발생되면 대규모 캐시 미스 현상이 발생할 수 있다는 문제가 존재한다. 이를 해결하는 방법이 ‘안정해시’이다.

## 안정 해시
**안정해시**란, **해시 테이블 크기가 조정될 때, 평균적으로 오직 k/n개의 키만 재배치하는 해시 기술**이다.


### 해시 공간과 해시 링

해시 링은, 해시 공간의 양쪽을 구부려 접은 구조이다. 해시 링 위해 서버와 키를 배치시키고 어떤 키가 저장되는 서버는, 해당 키의 위치로부터 시계 방향으로 링을 탐색해 나가면서 만나는 첫 번째 서버다.  

![](https://velog.velcdn.com/images/yeoni_/post/b7de75d3-083a-42c5-b6ac-a632f6a50e40/image.png)
서버를 추가하더라도 키 가운데 일부만 재배치하면 된다. 제거도 동일한 방법으로 동작한다.

### 문제점

1. 서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기가 균등하게 유지하는 게 불가능하다.
    
    파티션 : 인접한 서버 사이의 해시 공간
    
2. 키의 균등 분포를 달성하기가 어렵다.

---

위의 문제를 해결하기 위해 제안된 기법이 가상 노드 또는 복제라 불리는 기법이다.

### 가상 노드

가상 노드는 실제 노드 또는 서버를 가리키는 노드로 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있다. 해시 링 위에 여러 개의 가상 노드를 배치하는 것으로 가상 노드의 개수를 늘리면 키의 분포는 점점 더 균등해진다. 키를 재배치하는 방법은 기본 구현법과 동일하다.

# 6장. 키-값 저장소 설계

키-값 저장소는 키-값 데이터베이스라고도 불리는 비 관계형 데이터베이스이다.

## 분산 키-값 저장소

### CAP 정리

CAP 정리는 **데이터 일관성(consistency), 가용성(availability), 파티션 감내(partition tolerance)라는 3가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다**는 정리이다.

- 데이터 일관성
분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 보아야 한다.
- 가용성
    
    분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
    
- 파티션 감내
    
    파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미한다. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작하여야 한다는 것을 뜻한다.
    

![](https://velog.velcdn.com/images/yeoni_/post/c993c4c6-3b8b-48d9-bb5c-44135c9f959e/image.png)

CAP 정리는 이들 가운데 어떤 두 가지를 충족하려면 나머지 하나는 반드시 희생되어야 한다는 것을 의미한다. 3가지 요구사항 가운데 어느 2가지를 만족하느냐에 따라 CA, CP, AP 이렇게 3개로 분류할 수 있다. 실세계에서는 네트워크 장애는 피할 수 없는 일로 여겨지기 때문에 분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계되어야 한다. 즉, 실세계에서는 CA 시스템은 존재하지 않는다.

### 실세계의 분산 시스템

분산 시스템은 파티션 문제를 피할 수 없다. 때문에 파티션 문제가 발생하면 우리는 일관성과 가용성 사이에서 하나를 선택해야 한다. 
만약 **일관성을 선택한다면(CP)** 데이터 불일치 문제를 피하기 위해 쓰기 연산을 중단시켜야 하는데 이렇게 되면 가용성이 깨진다. 은행권 시스템처럼 데이터 일관성이 중요한 도메인에서는 데이터 일관성을 양보하지 않는다. 
반대로 **가용성을 선택한다면(AP)** 데이터의 일관성이 깨지더라도 쓰기 연산을 허용해야 한다. 후에 파티션 문제가 해결된다면 새 데이터를 전송하는 식으로 해결한다.

이처럼 분산 키-값 저장소를 만들 대 그 요구사항에 맞도록 CAP 정리를 적용해야 한다. 이 문제에 대해 면접관과 상의하고 그 결론에 따라 시스템을 설계하도록 하자.

### 데이터 파티션

대규모 애플리케이션의 경우 전체 데이터를 한 대의 서버에 욱여넣는 것은 불가능하다. 가장 단순한 해결책은 데이터를 작은 파티션들로 분할한 다음 여러 대 서버에 저장하는 것이다. 데이터를 파티션 단위로 나눌 때는 다음 2가지 문제를 중요하게 따져봐야 한다.

- 데이터를 여러 서버에 고르게 분산할 수 있는가?
- 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가?

안정 해시를 이용하여 데이터를 파티션하면, 규모 확장 자동화(automatic scaling)와 각 서버의 용량에 맞게 가상 노드의 수를 조정할 수 있는 다양성(heterogeneity)의 장점이 존재한다.

### 데이터 다중화

높은 가용성과 안정성을 확보하기 위해서는 데이터를 비동기적으로 다중화(replication)할 필요가 있다. 어떤 키를 해시 링 위에 배치한 후, 그 지점으로부터 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관하는 것이다. 만약, 가상 노드를 사용한다면 선택한 N개의 노드가 실제 대응될 물리 서버의 개수(≤ N)와 일치하지 않을 수 있다.  위 문제를 피하기 위해서는 노드를 선택할 때 같은 물리 서버를 중복 선택하지 않도록 해야 한다.

### 비 일관성 해소 기법: 데이터 버저닝

데이터를 다중화하면 데이터 일관성이 깨질 위험이 있다. 버저닝과 벡터 시계는 이 문제를 해소하기 위해 등장한 기술이다.
버저닝은 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것을 의미한다. 각 버전의 데이터는 변경 불가능하다. 버전 사이의 충돌이 발생했을 경우, 벡터 시계를 사용해서 충돌을 해결한다. 벡터 시계는 [서버, 버전]의 순서쌍을 데이터에 매단 것이다. 어떤 버전이 선행 버전인지, 또는 다른 버전과 충돌이 있는지 판별하는데 쓰인다. 어떤 버전 X와 Y 사이에 충돌이 존재하는지 보려면 Y의 벡터 시계 구성요소 가운데 X의 벡터 시계 동일 서버 구성요소보다 작은 값을 갖는 것이 있는지 보면 된다. 
벡터 시계를 사용해 충돌을 감지하고 해소하는 방법에도 단점이 존재한다. 먼저, 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로 클라이언트 구현이 복잡해진다는 것이다. 두번째로는 [서버 : 버전] 순서쌍 개수가 굉장히 빨리 늘어난다는 것이다. 임계치를 정하고 그 이상으로 길이가 길어지면 벡터 시계에서 순서쌍을 제거하는 방식으로 해결할 수는 있지만 이 방법은 버전 간 선후 관계가 정확하게 결정될 수 없기 때문에 충돌 해소 과정의 효율성이 낮아지게 된다. 하지만 아마존에 따르면 실제 서비스에서 이런 문제가 벌어지는 것은 발견한 적이 없다고 한다. 즉, 대부분의 기업에서 벡터 시계는 적용해도 괜찮다. 

### 장애 감지

가십 프로토콜(gossip protocol)은 분산형 장애 감지 솔루션이다. 동작 과정은 아래와 같다.
각 노드는 멤버십 목록(멤버 ID와 박동 카운터 쌍의 목록)을 유지하고 주기적으로 자신의 박동 카운터를 증가시킨다. 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보내고 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신한다. 만약 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애 상태인 것으로 간주한다.

### 장애 처리

먼저, **일시적 장애 처리** 방법이다.
장애 상태인 서버를 무시하고 쓰기 연산을 수행할 W개의 서버와 읽기 연산을 수행할 R개의 서버를 해시 링에서 고른다. 장애 서버로 가는 요청은 다른 서버가 잠시 맡아 처리하고 해당 서버가 복구되었을 때 발생한 변경사항을 일괄 반영하여 데이터의 일관성을 보존한다. 이런 장애 처리 방안을 단서 후 임시 위탁(hinted handoff) 기법이라고 한다.

이어서 **영구 장애 처리** 방법이다.
영구적인 노드의 장애 상태를 처리하기 위해 반-엔트로피(anti-entropy) 프로토콜을 구현하여 사본들을 동기화할 것이다. 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터 양을 줄이기 위해서는 머클 트리를 사용한다. 
![](https://velog.velcdn.com/images/yeoni_/post/bf660b28-4840-4b10-9071-90a2247b7596/image.png)
키 공간을 버킷으로 나눈 후, 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 적용하여 해시 값을 계산한다. 버킷별로 해시값을 계산한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다. 자식 노드의 레이블로부터 새로운 해시 값을 계산하여, 이진 트리를 상향식으로 구성해 나간다. 만약 루트 노드의 해시 값이 일치한다면 두 서버는 같은 데이터를 갖는 것이다. 때문에 머클 트리를 사용하면 다른 데이터를 갖는 버킷을 찾을 수 있으므로 그 버킷들만 동기화하면 된다.